{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8liLRPWvKZUhTMVbuHoS4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevengiacalone/Python-workshop/blob/main/Session_5_MCMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markov-Chain Monte Carlo (MCMC) fitting"
      ],
      "metadata": {
        "id": "uWuWs4Pa-Thh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last week, we used SciPy to do optimization and estimate the parameters of different models. This technique of curve fitting works when our models only have a few parameters, but often times we will need to work with models that have *dozens* of parameters. In these cases, traditional optimization methods fail and we need to turn to more complex numerical techniques. Markov-Chain Monte Carlos are a popular class of algorithms that are useful for these types of problems. In this session, we will design our own MCMC algorithm in Python. We will also use the popular Python package emcee to fit the light curve of a transiting exoplanet in order to calculate its size and orbit period.\n",
        "\n",
        "\n",
        "An MCMC also has the advantage of being Bayesian in nature. This means that we can set limits on the values of the parameters we're fitting for using different distributions. According to Bayes' Theorem, the posterior probability of some model with parameters $\\theta$ given some data $x$ has the relation\n",
        "\n",
        "$$P(\\theta | x) \\propto P(\\theta) P(x | \\theta)$$\n",
        "\n",
        "where $P(\\theta)$ is the prior probability of the parameters $\\theta$ having some value and $P(x | \\theta)$ is the likelihood of of the data $x$ given some model with the parameters $\\theta$. We can set the probability density function for the prior $P (\\theta)$ to be whatever we want, which can give us much better constraints on the values of our parameters! We'll see an example of this below."
      ],
      "metadata": {
        "id": "Wy8k90nd-pTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metropolis-Hastings Algorithm\n",
        "\n",
        "There are many different MCMC algorithms, but one of the oldest (and arguably the simplest) is the Metropolis-Hastings algorithm. In this section, we'll code up this algorithm ourselves in Python. Here are the steps (from none other than [Wikipedia](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)):\n",
        "\n",
        "1. Define some function that is proportional to the desired probability density function $P(\\theta | x)$. In our case it will simply be $f (\\theta | x) = P (\\theta) P (x | \\theta)$, just like in Bayes' Theorem.\n",
        "\n",
        "2. Define our prior and likelihood functions. In this example, we will assume uniform priors where the prior probility is equal for all values in some range. For the likelihood function, we will use the Mean Squared Error function (the same one we used in the last session for curve fitting).\n",
        "\n",
        "3. Define a proposal function $g(\\theta^\\prime | \\theta_{t})$, which determines a new value of $\\theta$ (which we will call $\\theta^\\prime$) given $\\theta_t$. In our case, we will draw from a normal (Gaussian) distribution centered at $\\theta_t$.\n",
        "\n",
        "4. Select initial values for our parameters, $\\theta_{t=0}$.\n",
        "\n",
        "5. Propose a $\\theta_{t+1}$ using the proposal function $g(\\theta^\\prime | \\theta_{t})$.\n",
        "\n",
        "6. Calculate the acceptance ratio $\\alpha = f (\\theta^\\prime | x) /  f (\\theta_{t} | x)$.\n",
        "\n",
        "7. Either accept or reject $\\theta^\\prime$. Do do this, generate a random number $u$ between 0 and 1. If $u \\leq \\alpha$, accept $\\theta^\\prime$ and continue, with $\\theta_{t+1} = \\theta^\\prime$. If $u > \\alpha$, reject $\\theta^\\prime$ and try again with a new $\\theta^\\prime$.\n",
        "\n",
        "8. Continue this process until you've reached the desired number of steps $T$."
      ],
      "metadata": {
        "id": "tORh3iAC-rRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start, let's define a data set that we want to fit to. We'll work with a linear model here, for simplicity."
      ],
      "metadata": {
        "id": "ZT1Y_8UCG61t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)\n",
        "\n",
        "x = np.linspace(0, 1, 100)\n",
        "m_true = 0.5\n",
        "b_true = 1.0\n",
        "y = m_true * x + b_true + np.random.normal(0, 0.05, 100)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel(\"x\", fontsize=14)\n",
        "plt.ylabel(\"y\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8K2GyjycHLNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1 - 2\n",
        "\n",
        "We'll start by defining our function $f (\\theta | x) = P (\\theta) P (x | \\theta)$. We'll work in log space, since that allow us to add things together instead of multiplying them."
      ],
      "metadata": {
        "id": "Y1jBMuYQGsK_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDCg6C7S-Nda"
      },
      "outputs": [],
      "source": [
        "# first, the prior function\n",
        "def lnprior(theta):\n",
        "    m, b = theta\n",
        "    if (-5 < m < 5) & (-5 < b < 5):\n",
        "        return 0\n",
        "    else:\n",
        "        return -np.inf\n",
        "\n",
        "# next, the model we want to fit to and the corresponding likelihood function\n",
        "def model(theta, x):\n",
        "    m, b = theta\n",
        "    return m * x + b\n",
        "\n",
        "def lnlike(theta, x, y):\n",
        "    m, b = theta\n",
        "    lnL = -np.sum((model(theta, x) - y)**2)\n",
        "    return lnL\n",
        "\n",
        "# lastly, define the combined probability function\n",
        "def lnprob(theta, x, y):\n",
        "    lnp = lnprior(theta)\n",
        "    lnL = lnlike(theta, x, y)\n",
        "    return lnp + lnL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3\n",
        "\n",
        "Next, we'll define our proposal function that returns $\\theta^\\prime$ given $\\theta_t$. Recall that we'll use a Gaussian distribution for the proposal function $g (\\theta^\\prime | \\theta_t)$."
      ],
      "metadata": {
        "id": "IVzCb1NjKi8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use a Gaussian distribution with a standard deviation of 0.05 for both m and b\n",
        "def g(theta):\n",
        "    m, b = theta\n",
        "    m2 = np.random.normal(m, 0.1)\n",
        "    b2 = np.random.normal(b, 0.05)\n",
        "    thetaprime = (m2, b2)\n",
        "    return thetaprime"
      ],
      "metadata": {
        "id": "oO8_iyAnKh4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4\n",
        "\n",
        "Now we'll initialize some values for $m$ and $b$. In this example, we'll use 5 \"walkers\". Essentially, we will run the simulation for 5 different initial values and combine the results at the end."
      ],
      "metadata": {
        "id": "qa7Cf_m7MN55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# just draw random values for each\n",
        "m_initial = np.random.uniform(low=-2, high=2, size=5)\n",
        "b_initial = np.random.uniform(low=-2, high=2, size=5)"
      ],
      "metadata": {
        "id": "XOFa7zkyL6l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5 - 8\n",
        "\n",
        "Now we need to define our loops. We'll use a nested loop with 5 outer iterations (one for each walker) and 50000 inner iterations (i.e., 50000 steps per walker). For each walker step, we want to first calculate $\\theta^\\prime$ using the function we defined above. Then calculate $\\alpha$ and determine if $\\theta^\\prime$ has been accepted or rejected. If it is accepted, go to the next iteration. If it is rejected, repeat the calculation for the same step.\n",
        "\n",
        "I've gotten it set up for you below. Fill in the blank spots where indicated."
      ],
      "metadata": {
        "id": "JZkflSgaNJPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make arrays to store values of m and b at each step\n",
        "m_array = np.zeros([5,50000])\n",
        "b_array = np.zeros([5,50000])\n",
        "\n",
        "m_array[:,0] = m_initial\n",
        "b_array[:,0] = b_initial\n",
        "\n",
        "# outer loop (one iter per walker)\n",
        "for i in range(5):\n",
        "\n",
        "    # inner loop (one iter per step)\n",
        "    for j in range(1,50000):\n",
        "\n",
        "        theta = (m_array[i,j-1], b_array[i,j-1])\n",
        "\n",
        "        ### draw new values of m and b ###\n",
        "\n",
        "        ### draw u and calculate alpha and determine which is greater ###\n",
        "\n",
        "        ### if u <= alpha, set the new values of m and b and break the while loop by setting accept = True"
      ],
      "metadata": {
        "id": "jNvCFMA8M2hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice work! Let's visualize our results. The following histograms are the posterior distribution for our parameters, which us the best-fit values for $m$ and $b$ as well as their uncertainties. Let's remove the first 1000 steps from the arrays as \"burn in.\""
      ],
      "metadata": {
        "id": "UM_qK_0XjV47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(m_array[:,1000:].flatten(), bins=50)\n",
        "plt.xlabel(\"m\", fontsize=14)\n",
        "plt.ylabel(\"number\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "plt.hist(b_array[:,1000:].flatten(), bins=50)\n",
        "plt.xlabel(\"b\", fontsize=14)\n",
        "plt.ylabel(\"number\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w5S_V5_pWYG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"m =\", np.mean(m_array[:,1000:].flatten()), \"+/-\", np.std(m_array[:,1000:].flatten()))\n",
        "print(\"b =\", np.mean(b_array[:,1000:].flatten()), \"+/-\", np.std(b_array[:,1000:].flatten()))"
      ],
      "metadata": {
        "id": "BHql0FmIqJDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's another visualization of how these posteriors were generated:\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/d/de/Flowchart-of-Metropolis-Hastings-M-H-algorithm-for-the-parameter-estimation-using-the.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "CDtaYuiB3mT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a good interactive tool that can visualize this process, see [this page](https://chi-feng.github.io/mcmc-demo/app.html?algorithm=RandomWalkMH&target=banana)."
      ],
      "metadata": {
        "id": "qjsuFCjbjc1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also sample these arrays to see what kinds of lines our posteriors produce."
      ],
      "metadata": {
        "id": "G9mpGWqqqxx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idxs = np.random.randint(low=5000, high=9999, size=1000)\n",
        "\n",
        "plt.scatter(x, y, c='k')\n",
        "for i in range(1000):\n",
        "    this_m = m_array[0, sample_idxs[i]]\n",
        "    this_b = b_array[0, sample_idxs[i]]\n",
        "    this_model = model((this_m, this_b), x)\n",
        "    plt.plot(x, this_model, 'r-', alpha=0.05, zorder=0)\n",
        "plt.xlabel(\"x\", fontsize=14)\n",
        "plt.ylabel(\"y\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TOqmM7wfrAV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's repeat this exercise one more time, but using different priors on $m$ and $b$. Let's assume that we have some initial constraints on the values that let us set Gaussian priors instead of uniform priors. Run the cell below to see how this affects our constraints on $m$ and $b$ from the MCMC."
      ],
      "metadata": {
        "id": "CH5jkF1OyJIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write new prior function that uses Gaussian priors\n",
        "def lnprior(theta):\n",
        "    # assume we know b is 0.5 +/- 0.2 a priori\n",
        "    m, b = theta\n",
        "    mu_m = 0.5\n",
        "    sigma_m = 0.2\n",
        "    m_logprior = np.log(1.0/(np.sqrt(2*np.pi)*sigma_m))-0.5*(m-mu_m)**2/sigma_m**2\n",
        "    # assume we know b is 1 +/- 0.1 a priori\n",
        "    mu_b = 1\n",
        "    sigma_b = 0.1\n",
        "    b_logprior = np.log(1.0/(np.sqrt(2*np.pi)*sigma_b))-0.5*(b-mu_b)**2/sigma_b**2\n",
        "    return m_logprior + b_logprior\n",
        "\n",
        "# next three functions are unchanged\n",
        "def model(theta, x):\n",
        "    m, b = theta\n",
        "    return m * x + b\n",
        "\n",
        "def lnlike(theta, x, y):\n",
        "    m, b = theta\n",
        "    lnL = -np.sum((model(theta, x) - y)**2)\n",
        "    return lnL\n",
        "\n",
        "def lnprob(theta, x, y):\n",
        "    lnp = lnprior(theta)\n",
        "    lnL = lnlike(theta, x, y)\n",
        "    return lnp + lnL\n",
        "\n",
        "# make arrays to store values of m and b at each step\n",
        "m_array2 = np.zeros([5,50000])\n",
        "b_array2 = np.zeros([5,50000])\n",
        "\n",
        "m_array2[:,0] = m_initial\n",
        "b_array2[:,0] = b_initial\n",
        "\n",
        "# outer loop (one iter per walker)\n",
        "for i in range(5):\n",
        "\n",
        "    # inner loop (one iter per step)\n",
        "    for j in range(1,50000):\n",
        "\n",
        "        theta = (m_array2[i,j-1], b_array2[i,j-1])\n",
        "\n",
        "        ### draw new values of m and b ###\n",
        "        thetaprime = g(theta)\n",
        "\n",
        "        ### draw u and calculate alpha and determine which is greater ###\n",
        "        u = np.random.random()\n",
        "        alpha = np.exp(lnprob(thetaprime, x, y)) / np.exp(lnprob(theta, x, y))\n",
        "\n",
        "        ### if u <= alpha, set the new values of m and b and break the while loop by setting accept = True\n",
        "        if u <= alpha:\n",
        "            m_array2[i,j] = thetaprime[0]\n",
        "            b_array2[i,j] = thetaprime[1]\n",
        "        else:\n",
        "            m_array2[i,j] = theta[0]\n",
        "            b_array2[i,j] = theta[1]\n",
        "\n",
        "# print results\n",
        "print(\"m =\", np.mean(m_array2[:,1000:].flatten()), \"+/-\", np.std(m_array2[:,1000:].flatten()))\n",
        "print(\"b =\", np.mean(b_array2[:,1000:].flatten()), \"+/-\", np.std(b_array2[:,1000:].flatten()))"
      ],
      "metadata": {
        "id": "4U8pEw1FrYOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our constraints on $m$ and $b$ are now much tigher!\n",
        "\n",
        "Let's compare the posterior distributions for $m$ and $b$ in the two runs."
      ],
      "metadata": {
        "id": "dUdcsdP50rOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(m_array[:,1000:].flatten(), bins=50, histtype=\"step\", color=\"r\", label=\"Uniform prior\")\n",
        "plt.hist(m_array2[:,1000:].flatten(), bins=50, histtype=\"step\", color=\"b\", label=\"Gaussian prior\")\n",
        "plt.xlabel(\"m\", fontsize=14)\n",
        "plt.ylabel(\"number\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.hist(b_array[:,1000:].flatten(), bins=50, histtype=\"step\", color=\"r\", label=\"Uniform prior\")\n",
        "plt.hist(b_array2[:,1000:].flatten(), bins=50, histtype=\"step\", color=\"b\", label=\"Gaussian prior\")\n",
        "plt.xlabel(\"b\", fontsize=14)\n",
        "plt.ylabel(\"number\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7sv98Cyr7JmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise"
      ],
      "metadata": {
        "id": "UkNGJr2W2RUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, we'll use the Python package emcee to perform an MCMC fit of data from the Kepler space telescope, which searched for transiting planets around Sun-like stars. The system we will look at () has a hot Jupiter, a Jupiter-size planet orbiting very close to its host star. We will use an MCMC to calculate the size and orbital period of the planet precisely.\n",
        "\n",
        "First, let's install the Python packages we'll use."
      ],
      "metadata": {
        "id": "k_auFzbj2TbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emcee"
      ],
      "metadata": {
        "id": "llD9W_UmzoaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install corner"
      ],
      "metadata": {
        "id": "2H2JlJ-k2uEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install batman-package"
      ],
      "metadata": {
        "id": "2BUccS-K2ztE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data exploration\n",
        "\n",
        "Let's take a look at the data. We'll import the file \"Kepler-41.csv\" from the GitHub and plot it. The file has three columns: time, flux, and flux_err (i.e., the uncertainty of each flux point)."
      ],
      "metadata": {
        "id": "Ae01Wki6X1Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Kepler-41.csv\")\n",
        "x = df.time.values\n",
        "y = df.flux.values\n",
        "yerr = df.flux_err.values\n",
        "\n",
        "plt.plot(x, y, 'k-')\n",
        "plt.xlabel(\"Time (days)\", fontsize=14)\n",
        "plt.ylabel(\"Normalized Flux\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IH9iafAS29pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The big (~1%) dips in brightness are due to the hot Jupiter passing in front of the star! We can see visually that the planet has an orbital period of about 1.855 days and that the first transit in the data occurs at about 131.6 days. Let's phase-fold the data to that period to see what the transit looks like up close."
      ],
      "metadata": {
        "id": "35KBJN15Zfhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_guess = 1.855\n",
        "t0_guess = 131.6\n",
        "x_fold = (x - t0_guess + P_guess/2) % P_guess - P_guess/2\n",
        "\n",
        "plt.errorbar(x_fold, y, yerr, fmt='.')\n",
        "plt.xlabel(\"Time from Transit Midpoint (days)\", fontsize=14)\n",
        "plt.ylabel(\"Normalized Flux\", fontsize=14)\n",
        "plt.xlim([-0.25,0.25])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L8ZVKWQSZEHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our initial guesses aren't perfect. We'll get better estimates using the MCMC."
      ],
      "metadata": {
        "id": "vzhMiUd9bvw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modeling transits with BATMAN\n",
        "\n",
        "BATMAN is a Python packages used to model exoplanet transits. You don't need to know much about this, other than the fact that it is the model we'll be fitting the data to. Our model will have six parameters:\n",
        "\n",
        "- $P_{\\rm orb}$: The orbital period of the planet (in days)\n",
        "- $T_0$: The time of the midpoint of the first transit (in days)\n",
        "- $R_p$: The size of the planet (in Earth radii)\n",
        "- $i$: The inclination of the planet orbit (in degrees)\n",
        "- $u_1$: A parameter describing the limb darkening of the star\n",
        "- $u_2$: Another parameter describing the limb darkening of the star\n",
        "\n",
        "We'll hold the following quantities fixed (see the discovery paper [Santerne et al. 2011](https://ui.adsabs.harvard.edu/abs/2011A%26A...536A..70S/abstract)):\n",
        "\n",
        "- $R_\\star$: The radius of the host star, which we will set to $1.02 \\, R_\\odot$\n",
        "- $M_\\star$: The mass of the host star, which we will set to $1.12 \\, M_\\odot$\n",
        "- $e$: The orbital eccentricity of the planet, which we will set to 0\n",
        "\n",
        "The function below returns the flux of the star given the six parameters above and a time array (in days)."
      ],
      "metadata": {
        "id": "NjsuWvhDb1r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import batman\n",
        "\n",
        "G = 6.67e-8 # Uniersal Grav Constant in cgs units\n",
        "Rearth = 6.371e+8 # Earth radius in cm\n",
        "Rsun = 6.957e+10 # Sun radius in cm\n",
        "Msun = 1.989e+33 # Sun mass in g\n",
        "Rstar = 1.02 * Rsun\n",
        "Mstar = 1.12 * Msun\n",
        "\n",
        "params = batman.TransitParams()\n",
        "\n",
        "def model(theta, x):\n",
        "    Porb, T0, Rp, i, u1, u2 = theta\n",
        "\n",
        "    params.t0 = T0                                                      #time of inferior conjunction\n",
        "    params.per = Porb                                                   #orbital period\n",
        "    params.rp = Rp*Rearth/Rstar                                         #planet radius (in units of stellar radii)\n",
        "    params.a = ((G*Mstar)/(4*np.pi**2)*(Porb*86400)**2)**(1/3) / Rstar  #semi-major axis (in units of stellar radii)\n",
        "    params.inc = i                                                      #orbital inclination (in degrees)\n",
        "    params.ecc = 0.                                                     #eccentricity\n",
        "    params.w = 90.                                                      #longitude of periastron (in degrees)\n",
        "    params.u = [u1, u2]                                                 #limb darkening coefficients [u1, u2]\n",
        "    params.limb_dark = \"quadratic\"                                      #limb darkening model\n",
        "\n",
        "    m = batman.TransitModel(params, x)    #initializes model\n",
        "    flux = m.light_curve(params)          #calculates light curve\n",
        "\n",
        "    return flux"
      ],
      "metadata": {
        "id": "9aas4KBzaJyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_time = np.linspace(131, 132, 1000)\n",
        "Rp_guess = 11\n",
        "i_guess = 90\n",
        "u1_guess = 0.2\n",
        "u2_guess = 0.2\n",
        "test_flux = model((P_guess, t0_guess, Rp_guess, i_guess, u1_guess, u2_guess), test_time)\n",
        "\n",
        "\n",
        "plt.plot(test_time, test_flux)\n",
        "plt.xlabel(\"Time (days)\", fontsize=14)\n",
        "plt.ylabel(\"Normalized Flux\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Se_ijVKff1sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Running MCMC using emcee\n",
        "\n",
        "emcee is a Python package that implements the MCMC we wrote above, along with some extra features. Functionally, it is pretty similar to what we did. First, we need to define our log prior, log likelihood, and log probability functions. Let's do the folling for each below:\n",
        "\n",
        "- lnprior -- Use the following priors\n",
        "    - $P_{\\rm orb}$: A Gaussian prior centered on 1.855 days and with a standard deviation of 0.01 days.\n",
        "    - $T_0$: A uniform prior between 131.5 days and 131.7 days (inclusive).\n",
        "    - $R_{\\rm p}$: A uniform prior between 1 Earth radius and 20 Earth radii (inclusive).\n",
        "    - $i$: A uniform prior between 70 degrees and 90 degrees (inclusive).\n",
        "    - $u_1$ and $u_2$: Uniform priors between -1 and 1 (inclusive).\n",
        "\n",
        "- lnlike -- Use the following likelihood function:\n",
        "\n",
        "$$ \\log{L} = -0.5 ∑ (({\\rm model} - y)^2 / y{\\rm err}^2) $$\n",
        "\n",
        "- lnprob -- Use the same function that we used earlier. That is, express the log of the posterior probability as the sum of the log prior and log likelihood."
      ],
      "metadata": {
        "id": "cC0w4eU3gz-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lnprior(theta):\n",
        "    P, t0, Rp, i, u1, u2 = theta\n",
        "\n",
        "    mu_P = P_guess\n",
        "    sigma_P = 0.01\n",
        "    P_logprior = np.log(1.0/(np.sqrt(2*np.pi)*sigma_P))-0.5*(P-mu_P)**2/sigma_P**2\n",
        "\n",
        "    if (131.5 <= t0 <= 131.7) & (70 <= i <= 90) & (1 <= Rp <= 20) & (-1 <= u1 <= 1) & (-1 <= u2 <= 1):\n",
        "        return P_logprior\n",
        "    else:\n",
        "        return -np.inf\n",
        "\n",
        "def lnlike(theta, x, y, yerr):\n",
        "    lnL = -np.sum((model(theta, x) - y)**2 / yerr**2)\n",
        "    return lnL\n",
        "\n",
        "def lnprob(theta, x, y, yerr):\n",
        "    lnp = lnprior(theta)\n",
        "    lnL = lnlike(theta, x, y, yerr)\n",
        "    return lnp + lnL"
      ],
      "metadata": {
        "id": "m4uAjM6ziX7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've gotten those functions down, run the next cell. This will run the MCMC, which contains 12 walkers and 50000 steps."
      ],
      "metadata": {
        "id": "rnTBcuMQfxyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import emcee\n",
        "\n",
        "nwalkers = 12 # need to have at least twice as many walkers as free parameters\n",
        "ndim = 6 # this is the number of free parameters we're fitting for\n",
        "\n",
        "# set the initial guesses for each of the 12 walkers\n",
        "initial_guess = np.array([P_guess, t0_guess, Rp_guess, i_guess, u1_guess, u2_guess])\n",
        "pos = initial_guess + 1e-7 * np.random.randn(nwalkers, ndim)\n",
        "\n",
        "# initiate the MCMC\n",
        "sampler = emcee.EnsembleSampler(\n",
        "    nwalkers, ndim, lnprob, args=(x, y, yerr)\n",
        ")\n",
        "sampler.run_mcmc(pos, 50000, progress=True);"
      ],
      "metadata": {
        "id": "jxFFgz6RD4SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at our best-fit parameters. Are these consistent with those from the literature reported [here](https://exoplanetarchive.ipac.caltech.edu/overview/Kepler-41%20b#planet_Kepler-41-b_collapsible)?"
      ],
      "metadata": {
        "id": "t3AfFZRAezx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flat_samples = sampler.get_chain(discard=10000, thin=15, flat=True)\n",
        "\n",
        "print(\"Porb =\", np.mean(flat_samples[:,0]), \"+/-\", np.std(flat_samples[:,0]))\n",
        "print(\"T0 =\", np.mean(flat_samples[:,1]), \"+/-\", np.std(flat_samples[:,1]))\n",
        "print(\"Rp =\", np.mean(flat_samples[:,2]), \"+/-\", np.std(flat_samples[:,2]))\n",
        "print(\"i =\", np.mean(flat_samples[:,3]), \"+/-\", np.std(flat_samples[:,3]))\n",
        "print(\"u1 =\", np.mean(flat_samples[:,4]), \"+/-\", np.std(flat_samples[:,4]))\n",
        "print(\"u2 =\", np.mean(flat_samples[:,5]), \"+/-\", np.std(flat_samples[:,5]))"
      ],
      "metadata": {
        "id": "XVFMYwGbJPpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize our resulting posterior distributions using a \"corner\" plot. Corner plots are useful because they allow us to easily identify covariances between the parameters."
      ],
      "metadata": {
        "id": "RjhanhnSfB1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import corner\n",
        "\n",
        "labels=[\"$P_\\\\mathrm{orb}$ (days)\", \"$T_0$ (days)\", \"$R_\\\\mathrm{p}$ ($R_\\\\oplus$)\", \"$i$ (degrees)\", \"$u_1$\", \"$u_2$\"]\n",
        "\n",
        "fig = corner.corner(\n",
        "    flat_samples, labels=labels, quantiles=[0.16, 0.5, 0.84], show_titles=True, title_kwargs={\"fontsize\": 12},\n",
        ");"
      ],
      "metadata": {
        "id": "jKQbGHt4J5FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, let's plot samples from the posterior and compare them to the Kepler data."
      ],
      "metadata": {
        "id": "E4_wbxpcX3rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_fit = np.median(flat_samples[:,0])\n",
        "t0_fit = np.median(flat_samples[:,1])\n",
        "x_fold = (x - t0_fit + P_fit/2) % P_fit - P_fit/2\n",
        "\n",
        "inds = np.random.randint(len(flat_samples), size=100)\n",
        "for ind in inds:\n",
        "    sample = flat_samples[ind]\n",
        "    idx_sorted = np.argsort(x_fold)\n",
        "    plt.plot(x_fold[idx_sorted], model(sample, x)[idx_sorted], \"C1\", alpha=0.1)\n",
        "\n",
        "plt.errorbar(x_fold, y, yerr, fmt='.', color='k', zorder=0)\n",
        "plt.xlabel(\"Time from Transit Midpoint (days)\", fontsize=14)\n",
        "plt.ylabel(\"Normalized Flux\", fontsize=14)\n",
        "plt.xlim([-0.15,0.15])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6QsvC2rnLr9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some inconsistencies, but overall it looks pretty good!\n",
        "\n",
        "That concludes our sesson on MCMC fitting. If you want to mess around with emcee some more, check out the documentation [here](https://emcee.readthedocs.io/en/stable/). There are also other MCMC packages in python, such as [pymc](https://www.pymc.io/welcome.html) and [pystan](https://pystan.readthedocs.io/en/latest/). You may come across these in your research, so it's good to have them on your radar!"
      ],
      "metadata": {
        "id": "z9EZuZarai70"
      }
    }
  ]
}