{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPZm0WLadGWdYgTn9W1CPxx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevengiacalone/Python-workshop/blob/main/Session_4_SciPy_and_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SciPy"
      ],
      "metadata": {
        "id": "AjJ6ygchBhza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we'll be focusing on SciPy, a Python library for scientific and technical computing. We'll start by going over some of the useful functionalities of the library. Then we'll turn our attention to curve fitting, which is a method for estimating the parameters of a function based on data.\n",
        "\n",
        "More information about SciPy and its uses can be found here: https://docs.scipy.org/doc/scipy/tutorial/index.html. Some modules not discussed here are:\n",
        "- Statistics\n",
        "- Fourrier Transforms\n",
        "- Signal Processing\n",
        "- Linear Algebra\n",
        "- Multidimensional Image Processing"
      ],
      "metadata": {
        "id": "lfbUBBSpDoug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integration"
      ],
      "metadata": {
        "id": "dv0_PDLKJBs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `scipy.integrate` library contains several options for integrating functions in Python. Here, we'll focus on the simplest one: `quad`."
      ],
      "metadata": {
        "id": "7tDXRNNANkEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import integrate"
      ],
      "metadata": {
        "id": "17MyYHDxJD7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's define a simple function that we want to integrate. In this example, we'll go with\n",
        "\n",
        "$f(x) = A x^2$\n",
        "\n",
        "where $A$ is some constant. We know this integrates to\n",
        "\n",
        "$∫ f(x) dx = ∫ A x^2 dx = A x^3 / 3$.\n",
        "\n",
        "If we set $A$ eqal to 1 and integrate over the range [0, 1], we should get 0.33."
      ],
      "metadata": {
        "id": "Y8Y7aqW6VAaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x, A):\n",
        "    return A*x**2\n",
        "\n",
        "A = 1\n",
        "I = integrate.quad(\n",
        "    func = my_func, # define the function you want to integrate\n",
        "    a = 0,          # set the starting value you are integrating over\n",
        "    b = 1,          # set the ending value you are integrating over\n",
        "    args=(A)        # set the values of any other arguments in the function\n",
        "    )\n",
        "\n",
        "print(I[0])"
      ],
      "metadata": {
        "id": "bM3TLXFNQB-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also perform double integrals, triple integrals, etc using the `dblquad`, `tplquad`, and `nquad` functions. Let's try a double integral of the function\n",
        "\n",
        "$f(t,x) = \\frac{e^{-x t}}{t^n}$\n",
        "\n",
        "where $n$ is some integer greater than 0. Evaluating $t$ over [1, $∞$] and $x$ over [0, $∞$] gives us\n",
        "\n",
        "$∫_0^∞ ∫_1^∞ f(t,x) dt dx = \\int_0^∞ ∫_1^∞ \\frac{e^{-x t}}{t^n} dt dx = \\frac{1}{n}$."
      ],
      "metadata": {
        "id": "qdexnFTbWgUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def my_func(t, x, n):\n",
        "    return np.exp(-x*t)/t**n\n",
        "\n",
        "n = 4\n",
        "I = integrate.dblquad(\n",
        "    func = my_func,  # define the function you want to integrate\n",
        "    a = 0,           # set lower bound of x\n",
        "    b = np.inf,      # set upper bound of x\n",
        "    gfun = 1,        # set lower bound of t\n",
        "    hfun = np.inf,   # set upper bound of t\n",
        "    args=(n,)        # set value of additional argument\n",
        "    )\n",
        "\n",
        "print(I[0])"
      ],
      "metadata": {
        "id": "N5xoXtJzddrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpolation"
      ],
      "metadata": {
        "id": "_RTyHuUyJEP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes you will have data and points $x_1$ and $x_2$, but you really need to know the value in between those two points. In these circumstances, it is common to use interpolation to estimate this value. Here, we'll go over two types of interpolation: piecewise linear interpolation and cubic spline interpolation.\n",
        "\n",
        "In piecewise linear interpolation, you simply draw a straight line between two adjacent points and assume all values fall along that line. We can do this using NumPy using the `np.interp` function. Let's see an example."
      ],
      "metadata": {
        "id": "g2mrtz6zkcUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define some sparsely sampled data\n",
        "x = np.linspace(0, 10, 11)\n",
        "y = np.cos(-x**2 / 9.0)\n",
        "\n",
        "# define a new high-resolution x grid that you want to interpolate onto\n",
        "xnew = np.linspace(0, 10, 1001)\n",
        "\n",
        "# use np.interp to calculate y values at each new x value\n",
        "ynew = np.interp(\n",
        "    x = xnew,   # x grid you want to interpolate into\n",
        "    xp = x,     # x data you are interpolating between\n",
        "    fp = y      # y data you are interpolating between\n",
        "    )\n",
        "\n",
        "# plot the original and new data to see what it looks like\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(x, y, \"ko\", label=\"Original data\")\n",
        "plt.plot(xnew, ynew, \"r-\", label=\"Interpolated data\")\n",
        "plt.xlabel(\"x\", fontsize=14)\n",
        "plt.ylabel(\"y\", fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zbERA86KJGgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the piecewise linear interpolation doesn't fill in the gaps perfectly, but it is often used anyway because it is very fast. If we want to do an interpolation that matches the form of the data more closely, we can use a cubic spline interpolation."
      ],
      "metadata": {
        "id": "srJWb0xKn0HD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import CubicSpline\n",
        "\n",
        "# first, define the spline object\n",
        "spl = CubicSpline(x, y)\n",
        "\n",
        "# next, get the new y data like so\n",
        "ynew = spl(xnew)\n",
        "\n",
        "# and plot\n",
        "plt.plot(x, y, \"ko\", label=\"Original data\")\n",
        "plt.plot(xnew, ynew, \"r-\", label=\"Interpolated data\")\n",
        "plt.xlabel(\"x\", fontsize=14)\n",
        "plt.ylabel(\"y\", fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HJdDpLN5oSUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, this method gives a more realistic result."
      ],
      "metadata": {
        "id": "iTJ0DNcWom81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization"
      ],
      "metadata": {
        "id": "3bntCvR_KLUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SciPy comes with a ton of useful optimization function (which you can check out here: https://docs.scipy.org/doc/scipy/tutorial/optimize.html). In this section, we'll go over one of these functions: `optimize.curve_fit`.\n",
        "\n",
        "The `curve_fit` function performs a least-squares fit of some data. In short, it solves for the minimum of the following equation:\n",
        "\n",
        "$$ ∑ (y_i - f(x_i, Θ))^2 $$\n",
        "\n",
        "where $x_i$ and $y_i$ are the $x$ and $y$ values of the $i$-th data point and $f$ is the model you are trying to fit the data to, which has some number of parameters represented by $\\theta$. The curve_fit algorithm will determine what values of these parameters optimize the model so that it best fits the data. Let's see an example of this below, where we will fit some simulated data to a straight line."
      ],
      "metadata": {
        "id": "5o_3M7w0lLcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simulate some data with some random noise and plot it\n",
        "x_sim = np.arange(0,10,0.1)\n",
        "b = 1\n",
        "m = 0.5\n",
        "y_sim = m*x_sim + b + np.random.normal(0, 0.5, len(x_sim))\n",
        "\n",
        "plt.scatter(x_sim, y_sim)\n",
        "plt.xlabel(\"x\", fontsize=14)\n",
        "plt.ylabel(\"y\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4qAUap2LKPVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to define our model (function), which we will assume takes the form\n",
        "\n",
        "$$f(x) = m x + b$$"
      ],
      "metadata": {
        "id": "iLkjmsY6niu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x, m, b):\n",
        "    return m*x + b"
      ],
      "metadata": {
        "id": "Z6b2gNutneKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we can use the `optimize.curve_fit` function to estimate the values of $m$ and $b$. This function returns two arrays:\n",
        "- fitParams: the best-fit parameters of the model\n",
        "- fitCovariances: the covariance matrix from the fit, which can be used to estimate the uncertainties on the best-fit parameters"
      ],
      "metadata": {
        "id": "uQU77e0Mn2e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import optimize\n",
        "\n",
        "fitParams, fitCovariances = optimize.curve_fit(\n",
        "    f = my_func,   # the name of the model to be fit\n",
        "    xdata = x_sim, # the x data\n",
        "    ydata = y_sim  # the y data\n",
        ")"
      ],
      "metadata": {
        "id": "SaJlJVVQoAF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like we've reached convergence! Let's print the best-fit parameters and compare to the true parameters. Let's also plot the data along with the best-fit model."
      ],
      "metadata": {
        "id": "qoxWMsnzo9mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"m_true =\", m)\n",
        "print(\"b_true =\", b)\n",
        "print()\n",
        "print(\"m_fit =\", fitParams[0], \"+/-\", np.sqrt(np.diag(fitCovariances))[0])\n",
        "print(\"b_fit =\", fitParams[1], \"+/-\", np.sqrt(np.diag(fitCovariances))[1])\n",
        "print()\n",
        "\n",
        "plt.scatter(x_sim, y_sim, label=\"Data\")\n",
        "plt.plot(x_sim, my_func(x_sim, fitParams[0], fitParams[1]), \"k\", label=\"Best-fit model\")\n",
        "plt.xlabel(\"x\", fontsize=14)\n",
        "plt.ylabel(\"y\", fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EpeYVHIcpEpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise"
      ],
      "metadata": {
        "id": "mhs2lF_WKOzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, we're going to calculate the rate at which the universe is expanding (i.e., the Hubble constant) using the cosmic distance ladder. I'm sure some of you are thinking: \"What is the distance ladder?\" Allow me to explain.\n",
        "\n",
        "In astronomy, there are a few ways of determining the distances to various objects. The oldest method of caclulating distance is **parallax**. The parallax method takes advantage of the fact that as the Earth moves around the Sun, nearby stars will appear to move in the sky relative to more distant background stars. The closer the star is, the more it \"moves.\" Using the small angle approximation, we can calculate the distance to a star using the simple equation\n",
        "\n",
        "$p = \\frac{\\rm 1 \\, AU}{d}$\n",
        "\n",
        "where the parallax $p$ is in units of arcseconds and the distance $d$ is in units of parsecs (note that 1 parsec is approximately 3.26 lightyears).\n",
        "\n",
        "![](https://itu.physics.uiowa.edu/sites/itu.physics.uiowa.edu/files/2021-08/parallax-2_med.png)\n",
        "\n",
        "In the late 19th century and early 20th century, astronomers measured the distances of thousands of stars using this method. Some of these stars were **Cepheids**, a class of pulsating variable stars. Thanks to these parallax measurements, [Henrietta Swan Leavitt](https://en.wikipedia.org/wiki/Henrietta_Swan_Leavitt) found that the pulsation periods of Cepheids correlate directly with their intrinsic luminosities. This meant that Cepheids could be used as **standard candles**: as long as you could measure the pulsation period of a Cepheid (usually by measuring a light curve), you could figure out how far away it was.\n",
        "\n",
        "In 1924, [Edwin Hubble](https://en.wikipedia.org/wiki/Edwin_Hubble) used Cepheids in the Andromeda nebula to show that the universe extends beyond the Milky Way, and that some of the blurry objects astronomers were seeing through their telescopes were actually aggregates of billions of stars called galaxies. This was a huge deal at the time, as it settled what was known as the [Great Debate](https://en.wikipedia.org/wiki/Great_Debate_(astronomy)) about the size and nature of the universe. In 1929, measurements of Cepheid distances in other galaxies were used to show that the universe is expanding (i.e., [Hubble's Law](https://en.wikipedia.org/wiki/Hubble%27s_law)).\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/6/6c/Delta_Cephei_lightcurve.jpg)\n",
        "\n",
        "\n",
        "Much later, another standard candle was discovered: **Type 1a supernovae**. These are supernovae that occur when a white dwarf accretes mass from a close-in binary stars. When the mass of the white dwarf exceeds the [Chandrasekhar limit](https://en.wikipedia.org/wiki/Chandrasekhar_limit#:~:text=The%20Chandrasekhar%20limit%20(%2F%CB%8Ct%CA%83,2.765%C3%971030%20kg).), it goes unstable and explodes. Some detected Type 1a supernovae have occurred in galaxies with observable Cepheids (and therefore know distances), allowing us to determine relationships between characteristics of the Type 1a supernova (e.g., its observed brightness over time at different wavelengths) and its distance. These only occur once every ~500 years in any individual galaxy, but with access to thousands of galaxies we can find them fairly easily.\n",
        "\n",
        "Using Type 1a supernovae, we are able to measure the distances to galaxies that are much farther away than we could using Cepheids. Lastly, by combining galactic distance with galactic redshift (a measure of how fast the galaxy is moving away from us), we can measure the speed of the expansion of the universe as a function of distance. Because more distant objects are older, this tells us the acceleration of the expansion of the universe!\n",
        "\n",
        "**To summarize**: The distance latter works as follows:\n",
        "- We use parallax to measure the distances of stars. Some of these stars are Cepheid variables, which we can calibrate a period-distance relationship for based on the fact that specific periods correspond to specific intrinsic luminosities.\n",
        "- We use the Cepheid period-distance relationship to determine the distances to nearby galaxies. We have observed Type 1a supernovae in some of these nearby galaxies, allowing us to calibrate a limunosity-distance relationship for Type 1a supernovae.\n",
        "- Laslty, we use the Type 1a supernovae to determine the distances to even more far-away galaxies."
      ],
      "metadata": {
        "id": "0FrWeZ56404m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's move on with the exercise. We'll start by determining the period-luminosity relationship for Cepheids with known distances. First, let's load some data."
      ],
      "metadata": {
        "id": "VYq-lPtbI8SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from astropy.io import fits\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import optimize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "hdul = fits.open('R16T4.fit')\n",
        "df_all = pd.DataFrame(hdul[1].data)\n",
        "hdul.close()\n",
        "\n",
        "# this DataFrame includes Cepheids in multiple galaxies\n",
        "# let's exclude M31 (Andromeda) for now\n",
        "print(f\"Total number of Cepheids in table: {len(df_all)}\")\n",
        "df = df_all[df_all.Gal != 'M31  ']\n",
        "print(f\"Number of Cepheids in table excluding those in M31: {len(df)}\")\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "eytLPu6oKMys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore this data a little bit. We'll extract three values:\n",
        "- $m_W$: The Wesenheit magnitude of the star (essentially, the apparent brightness of the star)\n",
        "- $\\log_{10}P$: The log of the period of the Cepheid\n",
        "- O/H: The log of the metallicity of the Cepheid\n",
        "\n",
        "Let's start with just one galaxy: NGC 4258. This galaxy is unique because it hosts what is known as a \"megamaser\" that has allowed us to calculate its distance very precisely. Using the Cepheids in this galaxy, we can determine the distances to other galaxies with Cepheids very precisely.\n",
        "\n",
        "Let's start by plotting $m_W$ against $\\log_{10} P$. We'll also color the points according to O/H."
      ],
      "metadata": {
        "id": "VREXJC7XzL13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R = 0.39\n",
        "mW = df['F160W'] - R*df['V-I']\n",
        "\n",
        "mW_ngc4258 = mW[df.Gal == \"N4258\"].values\n",
        "logP_ngc4258 = np.log10(df.Per[df.Gal == \"N4258\"].values)\n",
        "O_H_ngc4258 = df.__O_H_[df.Gal == \"N4258\"].values\n",
        "\n",
        "sc = plt.scatter(logP_ngc4258, mW_ngc4258, c=O_H_ngc4258)\n",
        "cb = plt.colorbar(sc, pad=0.01)\n",
        "cb.set_label('O/H', fontsize=14, rotation=270, labelpad=15)\n",
        "\n",
        "plt.ylim([26,20])\n",
        "plt.xlim([0.6,2.1])\n",
        "plt.ylabel('Wesenheit magnitude', fontsize=14)\n",
        "plt.xlabel('log$_{10}$(Period / days)', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "08AoO8i8yRac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's assume all of the Cepheids in this galaxy are approximately the same distance away from us and that $m_W$ depends only on $\\log_{10}P$ and O/H. This relationship can be described by the equation\n",
        "\n",
        "$$m_W = {\\rm zp} + b \\log_{10} P + Z ({\\rm O/H})$$\n",
        "\n",
        "where ${\\rm zp}$ is the \"zero point\" (y intercept) and $b$ and $Z$ are coefficients describing how much the brightness of the Cepheid changes with period and metallicity.\n",
        "\n",
        "##### Task 1\n",
        "\n",
        "Fill in the function below and then fit for ${\\rm zp}$, $b$, and $Z$ using the NGC 4258 data. Reproduce the plot above with the best-fit model overplotted."
      ],
      "metadata": {
        "id": "h4Xt0g41Grem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x, zp, b, Z):\n",
        "    logP, O_H = x # x is a 2d array where the first column is logP and the second is O_H\n",
        "    mW = # FINISH THIS LINE OF CODE\n",
        "    return mW"
      ],
      "metadata": {
        "id": "oRfqyIy_0xD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = np.array([logP_ngc4258, O_H_ngc4258])\n",
        "\n",
        "fitParams, fitCovariances = # FINISH THIS LINE OF CODE\n",
        "\n",
        "print('zp =', fitParams[0], \"+/-\", np.sqrt(np.diag(fitCovariances))[0])\n",
        "print('b =', fitParams[1], \"+/-\", np.sqrt(np.diag(fitCovariances))[1])\n",
        "print('Z =', fitParams[2], \"+/-\", np.sqrt(np.diag(fitCovariances))[2])"
      ],
      "metadata": {
        "id": "b3NgGZxh1xLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = plt.scatter(logP_ngc4258, mW_ngc4258, c=O_H_ngc4258, label=\"Data\")\n",
        "cb = plt.colorbar(sc, pad=0.01)\n",
        "cb.set_label('O/H', fontsize=14, rotation=270, labelpad=15)\n",
        "\n",
        "plt.plot(logP, my_func(x_data, fitParams[0], fitParams[1], fitParams[2]), \"k\", label=\"Best-fit model\")\n",
        "\n",
        "plt.ylim([25,20])\n",
        "plt.xlim([0.6,2.1])\n",
        "plt.ylabel('Wesenheit magnitude', fontsize=14)\n",
        "plt.xlabel('log$_{10}$(Period / days)', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8o9MxHcH3p0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Task 2\n",
        "\n",
        "Now that we've determined these coefficients (which should be true for all Cepheids, regardless of galaxy), we can move on to the next step and calculate the distances to *all* of the galaxies in our table. In astronomy, we often express the distance of an object using the **distance modulus ($\\mu$)**, which is given by\n",
        "\n",
        "$\\mu = m - M = 5 \\log_{10} \\frac{d}{10 \\, {\\rm pc}}$\n",
        "\n",
        "where $m$ is the apparent magnitude and $M$ is the absolute magnitude (a proxy for intrinsic luminosity).\n",
        "\n",
        "We can calculate $\\mu$ for each galaxy in our table using the following equation\n",
        "\n",
        "$$m_W = (\\mu - \\mu_{\\rm ngc4258}) + {\\rm zp} + b \\log_{10} P + Z ({\\rm O/H})$$\n",
        "\n",
        "Using a for loop, calculate $\\mu$ for each galaxy in the table. Assume the values of ${\\rm zp}$, $b$, and $Z$ are equal to those calculated in the previous part. Assume the distance modulus for NGC 4258 is $\\mu_{\\rm ngs4258} = 29.387$. Print and save the value of $\\mu$ for each galaxy (it should be close to 30 for each)."
      ],
      "metadata": {
        "id": "qfuvPEdzJuEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x, mu):\n",
        "    logP, O_H = x\n",
        "    mW = # FINISH THIS FUNCTION\n",
        "    return mW"
      ],
      "metadata": {
        "id": "kjXjSWHHSSUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "galaxies = ['M101 ', 'N1015', 'N1309', 'N1365', 'N1448', 'N2442', 'N3021', 'N3370', 'N3447', 'N3972',\n",
        "            'N3982', 'N4038', 'N4424', 'N4536', 'N4639', 'N5584', 'N5917', 'N7250', 'U9391']\n",
        "distance_moduli = np.zeros(len(galaxies))\n",
        "\n",
        "for i in range(len(galaxies)):\n",
        "    this_galaxy = galaxies[i]\n",
        "    mW_i = mW[df.Gal == this_galaxy].values\n",
        "    logP_i = np.log10(df.Per[df.Gal == this_galaxy].values)\n",
        "    O_H_i = df.__O_H_[df.Gal == this_galaxy].values\n",
        "\n",
        "    x_data_i = np.array([logP_i, O_H_i])\n",
        "\n",
        "    ### FINISH CODE BELOW TO CALCULATE DISTANCE MODULUS FOR EACH GALAXY ###\n",
        "\n",
        "    fitParams, fitCovariances = #\n",
        "    this_mu = #\n",
        "\n",
        "    #######################################################################\n",
        "\n",
        "    # print the distance modulus\n",
        "    print(\"mu for\", this_galaxy, \"is\", this_mu)\n",
        "\n",
        "    # save distance modulus to array\n",
        "    distance_moduli[i] = this_mu"
      ],
      "metadata": {
        "id": "NEV7vcZ34NYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Task 3\n",
        "\n",
        "Now that we've determined the distances to these galaxies, we can start talking about Type Ia supernovae. Each of these galaxies has has a Type Ia supernova detected in it with some apparent brightness $m_B$. In this task, we will determine the $B$-band absolute magnitude ($M_B$) of all Type Ia supernovae by combining these distances with their apparent brightnesses.\n",
        "\n",
        "To do this, use the definition of the distance modulus given above. You should find $M_B$ to be approximately -19.29."
      ],
      "metadata": {
        "id": "nk12A0fkXA3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supernova data from Riess et al. (2016)\n",
        "aB = 0.71273\n",
        "Tab5 = np.array([13.310, 17.015, 16.756, 15.482, 15.765, 15.840, 16.527, 16.476, 16.265, 16.048,\n",
        "                 15.795, 15.797, 15.110, 15.177, 15.983, 16.265, 16.572,  15.867, 17.034])\n",
        "mB = Tab5-5*aB"
      ],
      "metadata": {
        "id": "TUo2wPOaV8LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(m, M):\n",
        "    mu = # FINISH THE FUNCTION\n",
        "    return mu"
      ],
      "metadata": {
        "id": "iqQTZhF6YC_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CALCULATE AND PRINT MB USING optimize.curve_fit"
      ],
      "metadata": {
        "id": "4EC8SDEbY4Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Task 4\n",
        "\n",
        "Now that we know the distance to each galaxy and the abosolute magnitude of a Tyle Ia supernova, we can calculate the Hubble constant.\n",
        "\n",
        "\n",
        "In the next cell I load in data from over 200 Type Ia supernovae, which come from the papers [Betoule et al. (2014)](https://www.aanda.org/articles/aa/pdf/2014/08/aa23413-14.pdf) and [Hicken et al. (2009)](https://iopscience.iop.org/article/10.1088/0004-637X/700/2/1097/pdf). You can ignore  the contents of this cell and just run it."
      ],
      "metadata": {
        "id": "eqmugI7nZ2U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hdul = fits.open('B14TF3.fit')\n",
        "df_hubble = pd.DataFrame(hdul[1].data)\n",
        "hdul.close()\n",
        "df_1 = df_hubble[(df_hubble.zcmb > 0.0233) & (df_hubble.zcmb < 0.15)]\n",
        "\n",
        "hdul = fits.open('H9T2.fit')\n",
        "df_hubble = pd.DataFrame(hdul[1].data)\n",
        "hdul.close()\n",
        "df_2 = df_hubble[(df_hubble.zCMB > 0.0233) & (df_hubble.zCMB < 0.15)]\n",
        "\n",
        "temp = np.array(df_1.Name)[102:]\n",
        "B14TF3_names = []\n",
        "for name in temp:\n",
        "    B14TF3_names.append(\"\".join(name.split()))\n",
        "# print(B14TF3_names)\n",
        "\n",
        "temp = np.array(df_2.SimbadName)\n",
        "H9T2_names = []\n",
        "for name in temp:\n",
        "    lowercase = name.lower()\n",
        "    H9T2_names.append(\"\".join(lowercase.split()))\n",
        "# print(H9T2_names)\n",
        "\n",
        "common_names = np.intersect1d(B14TF3_names, H9T2_names)\n",
        "# print('Detections in both datasets:', common_names)\n",
        "mask = np.zeros(len(H9T2_names))\n",
        "for i in range(len(H9T2_names)):\n",
        "    if H9T2_names[i] in common_names:\n",
        "        mask[i] = 0\n",
        "    else:\n",
        "        mask[i] = 1\n",
        "\n",
        "# mask H9T2 dataframe so it only has the unique detections\n",
        "df_3 = df_2[mask == 1.]\n",
        "\n",
        "y = np.concatenate((0.2*df_1.mb, 0.2*df_3.Bmag))\n",
        "yerr = np.concatenate((0.2*df_1.e_mb, 0.2*df_3.e_Bmag))\n",
        "q0 = -0.55\n",
        "x = np.concatenate((np.log10(df_1.zcmb*3e5*(1 + 0.5*(1-q0)*df_1.zcmb - (1/6)*(1-q0-3*q0**2+1)*df_1.zcmb**2)), np.log10(df_3.zCMB*3e5*(1 + 0.5*(1-q0)*df_3.zCMB - (1/6)*(1-q0-3*q0**2+1)*df_3.zCMB**2))))"
      ],
      "metadata": {
        "id": "46y7pcGuZpvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have some data (defined as `x` and `y`). Don't worry too much about what these variables represent for now, just know that they can be used to find $\\log_{10}H_0$, where $H_0$ is the Hubble constant in units of km/s/Mpc. If you want to do a deep-dive, take a look at [Reiss et al. (2016)](https://ui.adsabs.harvard.edu/abs/2016ApJ...826...56R/abstract).\n",
        "\n",
        "Using optimize.curve_fit, calculate the Hubble constant from this data. The equation for finding $\\log_{10} H_0$ is as follows:\n",
        "\n",
        "$$ y = 0.2 M_B + x + 5 - \\log_{10} H_0 $$\n",
        "\n",
        "What value do you get? Does it agree with the value measured from the Cosmic Microwave Background from the Planck mission ($H_0 = 66.93 ± 0.62$ km/s/Mpc)?"
      ],
      "metadata": {
        "id": "-C6I3Sibc1xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You're on your own with this one!"
      ],
      "metadata": {
        "id": "9mnyLIu3awtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "amJveSLbkX8c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}